import type { Project } from "@/features/portfolio/types/projects";

export const PROJECTS: Project[] = [
  {
    id: "solartrack",
    title: "SolarTrack - AI-Powered Solar Analytics Platform",
    period: {
      start: "MM.YYYY", // TODO: Replace with actual start date (e.g., "01.2025")
    },
    link: "https://github.com/YOUR_USERNAME/solartrack", // TODO: Replace with actual GitHub URL
    demoLink: "https://solartrack-demo.vercel.app", // TODO: Replace with actual demo URL
    skills: [
      "Next.js 15",
      "React 19",
      "Three.js",
      "FastAPI",
      "Python",
      "Supabase",
      "PostgreSQL",
      "Docker",
      "Gemini Vision API",
      "JWT Auth",
      "Row-Level Security",
      "TypeScript",
      "Tailwind CSS",
      "React Three Fiber",
      "SQLAlchemy",
    ],
    summary: `Full-stack SaaS platform combining AI/ML vision models for automated handwritten log digitization with advanced backend engineering for secure multi-tenant data management. Implements vision-based OCR with Gemini, natural language query generation with AST validation, and family-based collaboration with Row-Level Security enforcing database-level authorization.`,
    description: `Architected production-grade full-stack SaaS with AI/ML pipeline: Gemini Vision API processes handwritten solar logbook photographs, extracts structured readings with multi-format date normalization (MM/DD/YY, YYYY-MM-DD variants), and returns validated JSON through Pydantic models. Engineered natural language SQL interface enabling users to query data in plain Englishâ€”AI converts queries to SELECT statements, validates through Abstract Syntax Tree parsing, appends parameterized user filters, and automatically selects visualization type (line/stat/heatmap), preventing SQL injection through defense-in-depth validation. Designed production-ready SDE architecture: monorepo with pnpm workspaces and shared TypeScript types across frontend/backend, provider-agnostic AI abstraction supporting Gemini/OpenAI/Claude/local models without code changes, SQLAlchemy ORM supporting both PostgreSQL and Oracle databases, and row-level security policies enforcing multi-tenant isolation at the database layer. Built JWT authentication, rate limiting on AI endpoints, CORS configuration, and family collaboration system with invite tokens and membership tracking.`,
    logo: "ST", // TODO: Replace with actual logo path or keep text abbreviation
    image: "/r/SolarTrack_Preview.png", // TODO: Add actual screenshot to /public/r/ directory
    isExpanded: true,
    isVisible: false,
  },
  {
    id: "cyberintel-summarizer",
    title: "CyberIntel Summarizer: Real-Time Threat Intelligence System",
    period: {
      start: "09.2024",
    },
    link: "https://github.com/PRawat00",
    demoLink: "https://cd.prwt.dev",
    skills: [
      "LoRA",
      "vLLM",
      "4-bit Quantization",
      "FastAPI",
      "PostgreSQL",
      "Streamlit",
      "Dynamic Batching",
      "Paged Attention",
      "NVD",
      "CISA",
      "MITRE ATT&CK",
    ],
    summary: `Real-time cybersecurity threat intelligence system analyzing 100+ daily CVE updates from NVD, CISA, and MITRE ATT&CK feeds. Features LoRA-fine-tuned LLM with 4-bit quantization achieving 3x throughput improvement and interactive Streamlit dashboard for threat analytics.`,
    description: `Engineered end-to-end cybersecurity intelligence pipeline ingesting NVD, CISA, and MITRE ATT&CK feeds, generating LLM-powered summaries and severity classifications for 100+ daily CVE updates with automated data monitoring. Deployed LoRA-fine-tuned model with 4-bit quantization using vLLM, achieving 3x throughput improvement and 60% memory reduction compared to FP16 baseline through dynamic batching and paged attention. Built production-ready API (FastAPI, PostgreSQL) with Streamlit dashboard displaying real-time threat analytics by severity, attack vector, and vendor, with interactive performance benchmarking across inference configurations.`,
    logo: "CI",
    image: "/r/CyberIntel__.gif",
    isExpanded: true,
  },
  {
    id: "wegmans-capstone",
    title: "Gluten Sensitivity Prediction System (Wegmans Capstone)",
    period: {
      start: "08.2024",
      end: "12.2024",
    },
    link: "https://github.com/PRawat00",
    skills: [
      "XGBoost",
      "Feature Engineering",
      "Class Imbalance",
      "Threshold Optimization",
      "Cost-Sensitive Learning",
      "F1-optimal",
      "Youden's J",
      "PR-AUC",
      "Business Analytics",
    ],
    summary: `XGBoost classification system analyzing 5.6M transaction records for Wegmans Food Market. Implemented threshold optimization improving precision by 49% while demonstrating reduced coupon waste and higher marketing ROI.`,
    description: `Built XGBoost classification models on 5.6M transaction records to predict gluten sensitivity, feature engineering (purchase cadence, brand preferences, substitution patterns) and handling severe class imbalance using undersampling and cost-weighted learning. Designed a threshold optimization framework across 4 strategies (F1-optimal, Youden's J, cost-weighted FN:FP, PR-AUC), improving precision by 49% while maintaining sensitivity for imbalanced classification. Conducted cost-sensitive analysis to quantify business trade-offs between false positives and false negatives, demonstrating reduced coupon waste and higher marketing ROI through optimized targeting.`,
    logo: "/images/companies/Wegamans_logo.jpeg",
    image: "/r/Wegmans_Image.png",
    isExpanded: false,
    showGithubLink: false,
    showDemoLink: false,
  },

  // Temporarily commented out projects - can be uncommented when needed
  /*
  {
    id: "realtime-tweet-sentiment",
    title: "Real-Time Tweet Sentiment Analysis Pipeline",
    period: {
      start: "01.2025",
      end: "03.2025",
    },
    link: "https://github.com/PRawat00",
    skills: [
      "Apache Spark Streaming",
      "Delta Lake",
      "Hugging Face Transformers",
      "MLflow",
      "Databricks",
      "Grafana",
      "Data Quality Validation",
      "Medallion Architecture",
      "Auto-scaling",
      "Real-time Processing",
    ],
    summary: `Real-time sentiment analysis pipeline processing 50K+ tweets per hour with 99.5% uptime. MLflow-packaged Hugging Face transformer achieving 92% accuracy with sub-200ms latency and Grafana monitoring for sentiment anomalies.`,
    description: `Comprehensive real-time data pipeline for analyzing tweet sentiment at scale, processing 50K+ tweets per hour through Bronze, Silver, Gold, and Application layers with 99.5% uptime. Deployed MLflow-packaged Hugging Face transformer achieving 92% accuracy with sub-200ms inference latency. Implemented auto-scaling Databricks clusters with optimized partitioning strategies, reducing query response times by 65% and enabling real-time aggregations across 1M+ tweet mentions. Engineered comprehensive monitoring system with Grafana dashboards and automated alerting for sentiment anomalies.`,
    logo: "/images/projects/realtime-tweet-sentiment.svg",
    image: "/images/projects/placeholders/data-pipeline.svg",
    isExpanded: false,
  },
  {
    id: "steam-insights",
    title: "Steam Insights (Gaming Market Analysis & Forecasting)",
    period: {
      start: "08.2024",
      end: "12.2024",
    },
    link: "https://github.com/PRawat00",
    skills: [
      "Apache Airflow",
      "Databricks Spark",
      "Kafka",
      "XGBoost",
      "Random Forest",
      "ARIMA",
      "Prophet",
      "Python",
    ],
    summary: `Gaming market analysis system processing 8M+ data points from 140K+ games with ETL pipeline using Airflow, Spark, and Kafka. Time series forecasting achieving 85% accuracy in genre demand predictions.`,
    description: `Comprehensive gaming market analysis and forecasting system processing 8M+ data points from 140K+ games. Built ETL pipeline with Apache Airflow, Databricks Spark, and Kafka. Developed ML models (XGBoost, Random Forest) for review analysis and pricing forecasts. Implemented time series forecasting (ARIMA, Prophet) achieving 85% accuracy in genre demand predictions and reliable sales forecasting.`,
    logo: "/images/projects/steam-insights.svg",
    image: "/images/projects/placeholders/gaming-analytics.svg",
    isExpanded: true,
  },
  {
    id: "finrag3",
    title: "FinRAG3 - Agentic Financial Document AI",
    period: {
      start: "02.2025",
    },
    link: "https://github.com/PRawat00",
    skills: [
      "LangGraph",
      "LangChain",
      "ChromaDB",
      "ColBERT v2",
      "FastAPI",
      "Docker",
      "GPU Computing",
    ],
    description: `6-phase agentic AI system for automated investment due diligence processing SEC filings (10-K, 10-Q) and fund prospectuses. Features custom parsing algorithms with 95% accuracy, multi-GPU optimization, and enterprise-grade MLOps pipeline. Reduces financial document analysis time from hours to 5 minutes.`,
    logo: "/images/projects/finrag3.svg",
    isExpanded: false,
  },
  {
    id: "hpc-chatbot",
    title: "HPC Documentation Assistant",
    period: {
      start: "02.2025",
    },
    link: "https://github.com/PRawat00",
    skills: [
      "RAG Systems",
      "Vector Databases",
      "FastAPI",
      "Docker",
      "HPC Systems",
      "Agentic AI",
    ],
    description: `RAG-based chatbot for High Performance Computing documentation automation. Features agentic AI for dynamic data scraping, vector database management for HPC queries, and automated user onboarding system for university research computing infrastructure.`,
    logo: "/images/projects/hpc-chatbot.svg",
    isExpanded: false,
  },
  {
    id: "health-analytics",
    title: "Health Analytics Platform",
    period: {
      start: "01.2023",
      end: "03.2023",
    },
    link: "https://github.com/PRawat00",
    skills: [
      "Django",
      "MongoDB",
      "LLaMA",
      "FactCC",
      "RAG Systems",
      "spaCy",
      "NLP",
      "Web Scraping",
    ],
    description: `End-to-end health analytics platform using Django and MongoDB for SSOT data collection. Implemented RAG approach with LLaMA and FactCC for accurate health summaries. Built NLP pipeline with spaCy and LSA for health trend extraction. Achieved 60% increase in data availability and 75% reduction in manual errors through adaptive web scraping.`,
    logo: "/images/projects/health-analytics.svg",
    isExpanded: false,
  },
  */
];
